---
title: "Report"
author: "Egar Garcia"
date: "1/22/2019"
output:
  pdf_document: default
  html_document: default
---

```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Overview

# Methodology

To evaluate the different (machine learning) methods tested in this report, an
object-oriented approach is used, each method is going to be represented 
by a model which would be generated through the construction of an object.

To generate a model, the constructor function receives a dataset (the training set) 
which is used to fit the model an construct its object.
The model's object includes a `predict` function used to perform a prediction
for another dataset (which could be the set for testing, validation, production, etc.).

For example, the following function creates and object to represent a model that always
gives the most common rate (the mode) of the training set as prediction:

```{r}
#' This object-constructor function is used to generate a model that returns 
#' a as prediction the most common rating in the dataset used to fit it.
#' @param dataset The dataset used to fit the model
#' @return The model
ModeModel <- function(dataset) {
  model <- list()

  model$ratings <- unique(dataset$rating)
  model$mode <- model$ratings[which.max(tabulate(match(dataset$rating, model$ratings)))]

  #' The prediction function
  #' @param s The dataset used to perform the prediction of
  #' @return A vector containing the prediction for the given dataset
  model$predict <- function(s) {
    model$mode
  }

  model
}
```


Using the constructor function, an object can be created to fit the particular model:

```{r}
model <- ModeModel(edx)
```


Then this model can be used to make predictions, p.e. the following code makes
predictions using the training and the validation sets:

```{r}
training_pred <- model$predict(edx)
validation_pred <- model$predict(validation)
```


And the predictions are helpful to messure the performance of the model in terms of
RMSE and/or accuracy, applied to the training and validation sets:

```{r}
sprintf("Train-RMSE: %f, Train-Acc: %f, Val-RMSE: %f, Val-Acc: %f",
        RMSE(training_pred, edx$rating),
        mean(training_pred == edx$rating),
        RMSE(validation_pred, validation$rating),
        mean(validation_pred == validation$rating))
```


## Analysing the data

Let's take an initial look at how the ratings are distributed by plotting an
histogram to account the amount of the different ratings given by the customers.

```{r}
edx %>%
  ggplot() +
  geom_histogram(aes(x = rating), binwidth = 0.25)
```


It can be observed that ratings can be interpreted as the number of starts from 1 to 5
that users give to a movie, however it can be seen that ratings ending with a half 
are used, at first impression looks like half start ratings are not very popular
among users.


Let's visualize the data from another point of view, this time plotting
the ratings against the timestamp. For exploratory purposes lets just plot
using a small subset of the dataset, since using the whole one might take a lot of
time and resources.

```{r, echo=FALSE}
set.seed(0)
```

```{r}
edx[createDataPartition(y = edx$rating, times = 1, p = 0.001, list = FALSE),] %>%
  ggplot(aes(x = as_datetime(timestamp), y = rating)) +
  geom_point() +
  labs(x = 'timestamp', y = 'rating')
```


Now something interesting can be observed, it looks like ratings ending in half-stars
were allowed after certain point in time, and before that time just full-stars were
allowed.

To find the point in time were ratings ending in half star started to appear,
the following code can be used. In basically gets the minimum timestamp in the dataset
where a rating with half star is found.

```{r}
half_stars_startpoint <- min(filter(edx, (rating * 2) %% 2 == 1)$timestamp)
```


Converting `half_stars_startpoint` to a more redable representation using the following
code:

```{r}
library(lubridate)
as_datetime(half_stars_startpoint)
```

It can be seen that the point in time when half-star ratings started to appear
was on `r as_datetime(half_stars_startpoint)`.


Again, let's plot the ratings against the timestamp, but this time adding a
vertical line indicating the point in time where half-star ratings were allowed.

```{r, echo=FALSE}
set.seed(0)
```

```{r}
edx[createDataPartition(y = edx$rating, times = 1, p = 0.001, list = FALSE),] %>%
  ggplot(aes(x = as_datetime(timestamp), y = rating)) +
  geom_point() +
  geom_vline(aes(xintercept = as_datetime(half_stars_startpoint)),
             color = "red", linetype = "dashed") +
  geom_text(aes(x = as_datetime(half_stars_startpoint),
                label = as_datetime(half_stars_startpoint),
                y = 2.5),
            color = "red", vjust = -1, angle = 90) +
  labs(x = 'timestamp', y = 'rating')
```


A clear partition of the dataset can be observed, the first one for ratings before 
`r as_datetime(half_stars_startpoint)` where only full-stars were allowed, 
and a second one for ratings after that point in time where half-stars were allowed.
Let's create another plot about the distribution of ratings in each one of these
partitions.

```{r}
partition_names = c(paste('before', as_datetime(half_stars_startpoint)),
                    paste('on or after', as_datetime(half_stars_startpoint)))

edx %>%
  mutate(partition = factor(ifelse(timestamp < half_stars_startpoint,
                                   partition_names[1], partition_names[2]),
                            levels = partition_names)) %>%
  ggplot() +
  geom_histogram(aes(x = rating), binwidth = 0.25) +
  facet_grid(~ partition)
```


Now the distribution of ratings looks very different, it seems that ratings using 
half-stars were also popular among users when they were allowed (i.e. in the second
partition).


Having a point in time to partition the dataset seems to be an important aspect to
consider, since it could mean a different users behavior from one partition to 
another.
For example, it might be that a partucular user had a tendency to rate movies with 
4 stars before `r as_datetime(half_stars_startpoint)`, but then after that date the 
same user might have changed its tendency to rate 3.5 instead, since now half-stars
were allowed.


## Refining the metodology to use partitioned models

Given the previous observation that the dataset can be significantly different
in two partitions (before `r as_datetime(half_stars_startpoint)` and on-or-after that),
it would be convenient to train and predict a particular model in the two separate
partitions and then merging the prediction results for the whole set.
The following function creates an object in charge of doing that,
for a given method it fits a modelfor each one of the partitions,
and it has a prediction function that merges the predictions 
for the first and second models according to the timestamp.


```{r}
#' This object-constructor function is used to generate a metamodel 
#' that contains two models,
#' one fitted for data before the startpoint when half stars were allowed in the
#' ratings, and the other one fitted for data on or after that startpoint.
#' The predictions are performed by choosing the appropriate model according to the 
#' data's timestamp.
#' 
#' @param dataset The dataset used to fit both models,
#'    it should contain a column called 'timestamp'
#' @param base_model_generator The function used to generate the base models,
#'    it should receive a dataset to fit the model and have a prediction function
#' @return The created metamodel
PartitionedModel <- function(dataset, base_model_generator) {
  partitioned_model <- list()

  # Spliting the dataset in 2,
  # one set for data before the startpoint when half stars were allowed
  dataset1 <- dataset %>% filter(timestamp < half_stars_startpoint)
  # the other one for the data on or after the startpoint when half stars were allowed
  dataset2 <- dataset %>% filter(timestamp >= half_stars_startpoint)

  # Generating a model for each dataset
  partitioned_model$model1 <- base_model_generator(dataset1)
  partitioned_model$model2 <- base_model_generator(dataset2)

  #' Performs a prediction with the combined fitted models,
  #' it tries to do the prediction with the respective model based on the timestamp,
  #' but if the prediction can not be performed then the other model is attempted.
  #' @param s The dataset used to perform the prediction of
  #' @return A vector containing the prediction for each row of the dataset
  partitioned_model$predict <- function(s) {
    # Performing the predictions on the whole dataset for each one of the models
    pred1 <- partitioned_model$model1$predict(s)
    pred2 <- partitioned_model$model2$predict(s)

    # Selecting the prediction to use according to the data's timestamp,
    # if a prediction is missing the prediction for the other model is used
    s %>%
      mutate(pred1 = pred1, pred2 = pred2) %>%
      mutate(pred = ifelse(timestamp < half_stars_startpoint,
                           ifelse(!is.na(pred1), pred1, pred2),
                           ifelse(!is.na(pred2), pred2, pred1))) %>%
      .$pred
  }
  
  partitioned_model
}
```


To meassure the accuracy it also would be convenient to have a function that 
rounds the predictions to the actual number to represent stars, either full or half.
That means that according to the timestamp, before the 
half-star starpoint only values in $\{1, 2, 3, 4, 5\}$ are allowed,
and on or after the startpoint values in $\{1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5\}$ 
are allowed. The folowing function performs such rounding:

```{r}
#' Converts a prediction (which is a floating point number) to a one used to 
#' represent ratings given by stars,
#' i.e. {1, 2, 3, 4, 5} if the timestamp is before the half start startpoint 
#' or {1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5} if the timestamp is on or after.
pred2stars <- function(timestamp, pred) {
  # Rounds the prediction either to be full-stars or having a half-star
  # according to the timestamp
  rounded_pred <- ifelse(timestamp < half_stars_startpoint,
                         round(pred),
                         round(pred * 2)/2)

  # Making sure the rating is not smaller that 1 or bigger than 5  
  min(max(rounded_pred, 1), 5)
}
```


To test a particular method if would be very helpful to have a function
that fits the respective model to either to the whole training set or the partitions
given by the half-star startpoint, and then meassures the prediction performance against
the training and validation sets. The following function does exaclty that and returns
the results in a dataset, which can be included as a table in this report.

```{r}
#' This function is used to report the performance of a model in terms of
#' RMSE and Accuracy for the training and validation sets.
#' It evaluates the performance in two modes:
#' 1) using the whole training set to fit the model and
#' 2) partitioning the training set before and on-or-after 
#' the startpoint when half stars were allowed.
#' @param model_generator The constructor function to generate the model
#' @returns A dataset reporting the performance results 
get_performance_metrics <- function(model_generator) {
  dataset_names <- c('Training', 'Validation')
  datasets <- c()
  modes <- c()
  rmses <- c()
  accuracies <- c()
  counter <- 0

  for (is_partitioned in c(FALSE, TRUE)) {
    # Chosing the mode PARTITIONED or WHOLE
    if (is_partitioned) {
      model <- PartitionedModel(edx, model_generator)
    } else {
      model <- model_generator(edx)
    }

    for (dataset_name in dataset_names) {
      # Chosing the dataset to evaluate
      if (dataset_name == 'Training') {
        ds <- edx
      } else {
        ds <- validation
      }

      counter <- counter + 1

      # Getting the prediction for the chosen dataset
      pred <- model$predict(ds)

      datasets[counter] <- dataset_name
      modes[counter] <- ifelse(is_partitioned, 'PARTITIONED', 'WHOLE')

      # Calculating the RMSE
      rmses[counter] <- RMSE(pred, ds$rating)
      # Calculating the accuracy
      accuracies[counter] <- mean(pred2stars(ds$timestamp, pred) == ds$rating)
    }
  }

  data.frame('Dataset' = datasets,
             'Mode' = modes,
             'RMSE' = rmses,
             'Accuracy' = accuracies)
}
```


## Putting all together

To get the performance of a specific (machine learning) method,
the command `get_performance_metrics(model_generator)`
would be used, where `model_generator` is the constructor function of the 
respective model.


For example, to get the performance of the previosly defined model that 
always predict the most common rating of the training set,
the following command calculates the prediction performance
and includes the results as a table in this report.

```{r results = 'asis'}
knitr::kable(get_performance_metrics(ModeModel),
             caption = "Results for the Mode based model")
```


# Methods

## Simple Average

```{r}
#' This object-constructor function is used to generate a model
#' that always returns as prediction the average of the rating in the
#' given dataset used to fit the model.
#' @param dataset The dataset used to fit the model
#' @return The model
AvgModel <- function(dataset) {
  model <- list()

  # The average of ratings
  model$mu <- mean(dataset$rating)

  #' The prediction function
  #' @param s The dataset used to perform the prediction of
  #' @return A vector containing the prediction
  model$predict <- function(s) {
    model$mu
  }

  model
}
```


```{r results = 'asis', echo=FALSE}
knitr::kable(get_performance_metrics(AvgModel),
             caption = "Results for the Average based model")
```


## Linear Like Model

See: <https://rafalab.github.io/dsbook/recommendation-systems.html>

```{r}
LinearLikeBiasBasedModel <- function(s) {
  model <- list()

  model$mu <- mean(s$rating)

  model$movie_info <- s %>%
    group_by(movieId) %>%
    summarise(movie_bias = mean(rating - model$mu))

  model$user_info <- s %>%
    left_join(model$movie_info, by = 'movieId') %>%
    group_by(userId) %>%
    summarise(user_bias = mean(rating - movie_bias - model$mu))

  model$predict <- function(t) {
    t %>%
      left_join(model$movie_info, by = 'movieId') %>%
      left_join(model$user_info, by = 'userId') %>%
      mutate(pred = model$mu +
               ifelse(!is.na(movie_bias), movie_bias, 0) +
               ifelse(!is.na(user_bias), user_bias, 0)) %>%
      .$pred
  }

  model
}
```

```{r results = 'asis', echo=FALSE}
knitr::kable(get_performance_metrics(LinearLikeBiasBasedModel),
             caption = "Results for the Average based model")
```


